{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 日本語Word2Vecサンプル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 青空文庫より「三四郎」をダウンロードし整形するまで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三四郎\n",
      "夏目漱石\n",
      "\n",
      "-------------------------------------------------------\n",
      "【テキスト中に現れる記号について】\n",
      "\n",
      "《》：ルビ\n",
      "（例）頓狂《とんきょう》\n",
      "\n",
      "｜：ルビの付く文字列の始まりを特定する記号\n",
      "（例）福岡県｜京都郡《みやこぐん》\n",
      "\n",
      "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\n",
      "　　　（数字は、JIS X 0213の面区点番号または\n",
      "\n",
      "一\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬい\n",
      "\n",
      "\n",
      "評に取りかかる。与次郎だけが三四郎のそばへ来た。\n",
      "「どうだ森の女は」\n",
      "「森の女という題が悪い」\n",
      "「じゃ、なんとすればよいんだ」\n",
      "　三四郎はなんとも答えなかった。ただ口の中で迷羊、迷羊と繰り返した。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zipファイルダウンロード\n",
    "url = 'https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip'\n",
    "zip = '794_ruby_4237.zip'\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(url, zip)\n",
    "\n",
    "# ダウンロードしたzipの解凍\n",
    "import zipfile\n",
    "with zipfile.ZipFile(zip, 'r') as myzip:\n",
    "    myzip.extractall()\n",
    "    # 解凍後のファイルからデータ読み込み\n",
    "    for myfile in myzip.infolist():\n",
    "        # 解凍後ファイル名取得\n",
    "        filename = myfile.filename\n",
    "        # ファイルオープン時にencodingを指定してsjisの変換をする\n",
    "        with open(filename, encoding='sjis') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "print(text[:200])\n",
    "\n",
    "# ファイル整形\n",
    "import re\n",
    "# ヘッダ部分の除去\n",
    "text = re.split('\\-{5,}',text)[2]\n",
    "# フッタ部分の除去\n",
    "text = re.split('底本：',text)[0]\n",
    "# | の除去\n",
    "text = text.replace('|', '')\n",
    "# ルビの削除\n",
    "text = re.sub('《.+?》', '', text)\n",
    "# 入力注の削除\n",
    "text = re.sub('［＃.+?］', '',text)\n",
    "# 空行の削除\n",
    "text = re.sub('\\n\\n', '\\n', text) \n",
    "text = re.sub('\\r', '', text)\n",
    "\n",
    "# 整形結果確認\n",
    "\n",
    "# 頭の100文字の表示 \n",
    "print(text[:100])\n",
    "# 見やすくするため、空行 \n",
    "print()\n",
    "print()\n",
    "# 後ろの100文字の表示 \n",
    "print(text[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Janomeを使い三四郎テキストから単語リストを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: janome in /miniconda3/lib/python3.7/site-packages (0.3.9)\n",
      "一\n",
      "する\n",
      "目\n",
      "さめる\n",
      "女\n",
      "隣\n",
      "じいさん\n",
      "話\n",
      "始める\n",
      "いる\n"
     ]
    }
   ],
   "source": [
    "# Janomeのインストール\n",
    "!pip install janome | tail -n 1\n",
    "\n",
    "# Janomeのロード\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenneizerインスタンスの生成 \n",
    "t = Tokenizer()\n",
    "\n",
    "# テキストを引数として、形態素解析の結果、名詞・動詞・形容詞の原形のみを配列で抽出する関数を定義 \n",
    "def extract_words(text):\n",
    "    tokens = t.tokenize(text)\n",
    "    return [token.base_form for token in tokens \n",
    "        if token.part_of_speech.split(',')[0] in['名詞', '動詞','形容詞']]\n",
    "\n",
    "#  関数テスト\n",
    "# ret = extract_words('三四郎は京都でちょっと用があって降りたついでに。')\n",
    "# for word in ret:\n",
    "#   print(word)\n",
    "\n",
    "# 全体のテキストを句点('。')で区切った配列にする。 \n",
    "sentences = text.split('。')\n",
    "# それぞれの文章を単語リストに変換(処理に数分かかります)\n",
    "word_list = [extract_words(sentence) for sentence in sentences]\n",
    "\n",
    "# 結果の一部を確認 \n",
    "for word in word_list[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 準備したデータを用いてWord2Vecのモデル作成、学習実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /miniconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.205->boto3->smart-open>=1.7.0->gensim) (2.8.0)\r\n"
     ]
    }
   ],
   "source": [
    "# Word2Vecライブラリの導入\n",
    "!pip install gensim | tail -n 1 \n",
    "\n",
    "# Word2Vecライブラリのロード\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# size: 圧縮次元数\n",
    "# min_count: 出現頻度の低いものをカットする\n",
    "# window: 前後の単語を拾う際の窓の広さを決める\n",
    "# iter: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
    "# model.wv.most_similarの結果が1に近いものばかりで、model.dict['wv']のベクトル値が小さい値ばかりの \n",
    "# ときは、学習回数が少ないと考えられます。\n",
    "# その場合、iterの値を大きくして、再度学習を行います。\n",
    "\n",
    "# 事前準備したword_listを使ってWord2Vecの学習実施\n",
    "model = word2vec.Word2Vec(word_list, size=100,min_count=5,window=5,iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1741808  -0.43168685 -0.65332335  0.6833014   0.57470125  0.2539509\n",
      "  0.12178501  0.32688227  0.10116928 -1.3088945   0.27169803 -0.5457899\n",
      "  1.1889824  -0.27545893 -0.4110091   0.04593495  0.7961927   0.10180763\n",
      "  1.1695373   0.20678271 -0.30108437 -0.13187823 -0.137792    0.02033613\n",
      "  0.4064897   0.04740463  0.95096415  0.2242543   0.3909797  -0.07125689\n",
      "  0.41575217 -0.6635489   0.3752715   0.29347792  0.06934108  0.49085644\n",
      " -0.22049908 -0.40814143  0.22617292  0.61900294  0.2793214  -0.4029485\n",
      " -0.40094432 -0.00381183  0.13973615 -0.86965793 -0.4124246  -0.00688193\n",
      " -0.08912339  0.5427453   0.3621293   0.3323013   0.01785805 -0.08497438\n",
      " -0.4373295   0.41017246 -0.25815445  0.08704685 -0.18028507  0.23353882\n",
      " -0.00399473  0.4531782   0.2497752   0.07789893 -0.6005639  -0.38498324\n",
      " -0.25364298  0.35326108  1.1815746   0.46302557 -0.1125634   0.3769569\n",
      "  0.41945183 -0.75088286  0.07509755 -0.2635251   0.3281033   0.06958311\n",
      "  0.06325916  0.24846655  0.53973997  0.41797686  0.3765159  -0.07018918\n",
      "  0.05473132 -0.30974784  0.67351544  0.62684995 -0.24494146  0.6829085\n",
      " -0.22961633  0.12344312 -0.03204637  0.56715834 -0.47878748  0.20769037\n",
      " -0.19429745  0.3671398  -0.6283211   0.04018482]\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認1\n",
    "# 一つ一つの単語は100次元のベクトルになっています。 \n",
    "# 「世間」のベクトル値を確認します。\n",
    "print(model.__dict__['wv']['世間'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外国 0.6087949872016907\n",
      "社会 0.5741201043128967\n",
      "自己 0.5520804524421692\n",
      "喝采 0.5300933122634888\n",
      "聞こえる 0.5240653157234192\n",
      "うる 0.5086288452148438\n",
      "決心 0.5039682388305664\n",
      "有す 0.4733140468597412\n",
      "界 0.4718446433544159\n",
      "堪える 0.46965426206588745\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認2\n",
    "# 関数most_similarを使って「世間」の類似単語を調べます \n",
    "ret = model.wv.most_similar(positive=['世間']) \n",
    "for item in ret:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
